💥💥💥7.23 [MasterYip ](https://github.com/MasterYip)students open sourced [ChatPaper2Xmind ](https://github.com/MasterYip/ChatPaper2Xmind)! Use Chat to generate brief XMind notes of pictures and formulas from paper PDFs with one click.

💥💥💥7.22 The files in the warehouse have been organized. There may be some paths and bugs, which are being fixed. Added new local PDF full-text translation function! [⛏️PDF full text translation configuration tutorial](https://github.com/kaixindelele/ChatPaper#%E4%BB%BB%E6%84%8Fpdf%E5%85%A8%E6%96%87%E7%BF%BB%E8%AF%91%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B)

Major historical updates

* 🌟 _2023.07.23 _: [MasterYip ](https://github.com/MasterYip)students open sourced [ChatPaper2Xmind ](https://github.com/MasterYip/ChatPaper2Xmind)! Generate brief XMind notes of pictures + formulas from paper PDFs through Chat with one click.
* 🌟 _2023.07.22 _: Added new local PDF full-text translation function! [⛏️PDF full text translation configuration tutorial](#bookmark=id.46r0co2)
* 🌟 _2023.07.21 _: The files in the warehouse have been organized. There may be some paths and bugs, which are being fixed.
* 🌟 _2023.07.09 _: Junior brother [red-tie ](https://github.com/red-tie)has optimized a [one-click literature review function based ](https://github.com/kaixindelele/ChatPaper/tree/main/auto_survey)on [auto-draft ](https://github.com/CCCBora/auto-draft). It is suitable for everyone to quickly master a specific field, and supports the direct generation of Chinese literature research reports. The file configuration is simple, everyone is welcome to use and give feedback!
* 🌟 _2023.07.05 _: Yesterday I made a new little toy: [ChatSensitiveWords ](https://github.com/kaixindelele/ChatSensitiveWords), which uses LLM+ sensitive vocabulary to automatically determine whether sensitive words are involved. It has been launched on the academic version of GPT web page, and LLM developers are welcome to work together to improve this work.
* 🌟 _2023.04.30 _: **The only official website: **[https://chatpaper.org/ ](https://chatpaper.org/), and novice tutorial [Novice tutorial on using ChatPaper web version - Bilibili] https://b23.tv/HpDkcBU, third-party documents: https://chatpaper.readthedocs.io .
* 🌟 _2023.04.22 _: In order to celebrate ChatPaper receiving 10,000⭐, we will join forces with two students to launch two AI-assisted literature summary tools. The first one is [auto-draft ](https://github.com/CCCBora/auto-draft), which uses AI to automatically collect and sort out literature summaries!
* 🌟 _2023.04.17 _: In order to reduce academic ethics risks, we have added complex text injection to Chat_Reviewer. The effect is as shown in the following figure: Example [picture ](https://github.com/kaixindelele/ChatPaper/blob/main/images/reviews.jpg). We hope that teachers and students must pay attention to academic ethics and academic reputation when using it, and do not abuse the tool. If anyone has a better way to limit the irregular use of a few people, please leave a message and make a contribution to the scientific research community.
* 🌟 _2023.03.31 _: 30,000+ CCF-A conference papers have been summarized offline so far, so you don’t have to wait so long in the future!
* 🌟 _2023.03.28 _: Rong Sheng released a very interesting work today, [ChatGenTitle ](https://github.com/WangRongsheng/ChatGenTitle), which provides abstract generation titles and the results of fine-tuning based on the data of the 220wArXiv paper!
* 🌟 _2023.03.23 _: chat_arxiv.py can directly crawl the latest field papers from the arxiv website, based on keywords, several papers in recent days! Solved the previous inaccurate search problem of arxiv packages!
* 🌟 _2023.03.23 _: ChatPaper finally becomes a completed form! Now there are functions such as paper summary + paper polishing + paper analysis and improvement suggestions + paper review reply and other functions!

ChatPaper

We hope that language will no longer be an obstacle for Chinese people to obtain the latest knowledge.

One-stop service/simple/fast/efficient video tutorial·online experience

ChatPaper accelerates scientific research throughout the entire process: paper summary + professional-level translation + polishing + review + review response

<table>
  <tr>
   <td>Tool name
   </td>
   <td>Tool role
   </td>
   <td>Are you online?
   </td>
   <td>online preview
   </td>
   <td>Remark
   </td>
  </tr>
  <tr>
   <td>ChatPaper
   </td>
   <td>through ChatGPT <strong>to help researchers conduct preliminary screening of papers.</strong>
   </td>
   <td>Visit <a href="https://chatpaper.org/">chatpaper.org </a>to use
   </td>
   <td>

    **All functions are free and the code is open source, so everyone can use it with confidence! **Regarding how to obtain the API, first you must have a ChatGPT account that has not been blocked, then[obtain the Api Key ](https://chatgpt.cn.obiscr.com/blog/posts/2023/How-to-get-api-key/)and fill it in!

## Table of contents:

* [💥Latest news](#bookmark=id.30j0zll)
* [💫Start the engine](#bookmark=id.1fob9te)
* [⛏️Configuration tutorial](#bookmark=id.2et92p0)
* [⛏️PDF full text translation configuration tutorial](https://github.com/kaixindelele/ChatPaper#%E4%BB%BB%E6%84%8Fpdf%E5%85%A8%E6%96%87%E7%BF%BB%E8%AF%91%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B)
* [👷‍♂️HuggingFace online deployment](#bookmark=id.2lwamvv)
* [📄Local PDF full text translation example](#bookmark=id.111kx3o)
* [📄Example of local PDF full text summary](#bookmark=id.3l18frh)
* [📄Example of full text summary](#bookmark=id.2u6wntf)
* [👁️‍🗨️Tips for use](#bookmark=id.19c6y18)
* [🛠️Common errors](#bookmark=id.3tbugp1)
* [💐Project Acknowledgments](#bookmark=id.28h4qwu)
* [🌟Sponsor us](#bookmark=id.206ipza)
* [🌈Starchart](#bookmark=id.4k668n3)
* [🏆Contributors](#bookmark=id.2zbgiuw)

## latest news

* 🌟 _2023.07.23 _: [MasterYip ](https://github.com/MasterYip)students open sourced [ChatPaper2Xmind ](https://github.com/MasterYip/ChatPaper2Xmind)! Generate brief XMind notes of pictures + formulas from paper PDFs through Chat with one click
* 🌟 _2023.07.22 _: Added new local PDF full-text translation function! [⛏️PDF full-text translation configuration tutorial ](#bookmark=id.46r0co2).
* 🌟 _2023.07.21 _: The files in the warehouse have been organized. There may be some paths and bugs, which are being fixed.
* 🌟 _2023.07.09 _: Junior brother [red-tie ](https://github.com/red-tie)has optimized a [one-click literature review function based ](https://github.com/kaixindelele/ChatPaper/tree/main/auto_survey)on [auto-draft ](https://github.com/CCCBora/auto-draft). It is suitable for everyone to quickly master a specific field, and supports the direct generation of Chinese literature research reports. The file configuration is simple, everyone is welcome to use and give feedback!
* 🌟 _2023.07.05 _: Yesterday I made a new little toy: [ChatSensitiveWords ](https://github.com/kaixindelele/ChatSensitiveWords), which uses LLM+ sensitive vocabulary to automatically determine whether sensitive words are involved. It has been launched on the academic version of GPT web page, and LLM developers are welcome to work together to improve this work.
* 🌟 _2023.04.30 _: **The only official website: **[https://chatpaper.org/ ](https://chatpaper.org/), and novice tutorial [Novice tutorial on using ChatPaper web version - Bilibili] https://b23.tv/HpDkcBU, third-party documents: https://chatpaper.readthedocs.io .
* 🌟 _2023.04.22 _: In order to celebrate ChatPaper receiving 10,000⭐, we will join forces with two students to launch two AI-assisted literature summary tools. The first one is [auto-draft ](https://github.com/CCCBora/auto-draft), which uses AI to automatically collect and sort out literature summaries!
* 🌟 _2023.04.17 _: In order to reduce academic ethics risks, we have added complex text injection to Chat_Reviewer. The effect is as shown in the following figure: Example [picture ](https://github.com/kaixindelele/ChatPaper/blob/main/images/reviews.jpg). We hope that teachers and students must pay attention to academic ethics and academic reputation when using it, and do not abuse the tool. If anyone has a better way to limit the irregular use of a few people, please leave a message and make a contribution to the scientific research community.
* 🌟 _2023.03.31 _: 30,000+ CCF-A conference papers have been summarized offline so far, so you don’t have to wait so long in the future!
* 🌟 _2023.03.28 _: Rong Sheng released a very interesting work today, [ChatGenTitle ](https://github.com/WangRongsheng/ChatGenTitle), which provides abstract generation titles and the results of fine-tuning based on the data of the 220wArXiv paper!
* 🌟 _2023.03.23 _: chat_arxiv.py can directly crawl the latest field papers from the arxiv website, based on keywords, several papers in recent days! Solved the previous inaccurate search problem of arxiv packages!
* 🌟 _2023.03.23 _: ChatPaper finally becomes a completed form! Now there are functions such as paper summary + paper polishing + paper analysis and improvement suggestions + paper review reply and other functions!

## Start the engine

Start engine details

Faced with the massive amount of arxiv papers every day and the rapid evolution of AI, we humans must also evolve together to avoid being eliminated.

As a doctoral student in reinforcement learning at the University of Science and Technology of China, I feel deeply anxious. The current rate of evolution of AI is beyond my imagination.

So I developed this ChatPaper to try and defeat magic with magic.

ChatPaper is a paper summary tool. The AI takes one minute to summarize the paper, and the user takes one minute to read the paper summarized by the AI.

It can automatically download the latest papers on arxiv based on the keywords entered by the user, and then use the powerful summary capability of the API interface of ChatGPT3.5 to summarize the papers into a fixed format with the least text and the lowest reading threshold. Everyone provides the maximum amount of information to decide which articles to read intensively.

You can also provide the local PDF document address and process it directly.

Generally, you can speed-read the latest articles in a small field in one night. I've been testing it myself for two days.

I wish you all can evolve with AI in this era of rapid change!

Everyone's sponsorship is welcome to help pay for the API and server costs of web page operation, and give us the motivation to continue to develop more and higher-quality services!

Your support is my motivation and appreciation for continuous updates!

Welcome everyone to join the glorious evolution!

## Technical principle:

Technical principle details

The paper summary follows the following four questions:

1. Research Background
2. What were the past plans? What's wrong with them?
3. What is the plan for this article? What are the specific steps?
4. In which tasks has this article achieved what results?

Basically it is the main content of everyone’s paper report.

Implementation details: Extract the content of the abstract and introduction, because the abstract rarely tells you what the past solutions were and what problems there were.

Then extract the method chapter and summarize the specific steps of the method

Finally, extract the conclusion chapter and summarize the full text.

Summarize and feed in three times. If each part exceeds the length, it will be truncated (this solution is too crude at the moment, but there is no better or more elegant solution)

As a preliminary screening, it is barely enough.

## Configuration tutorial

Configuration tutorial details

### 1. Run in script mode

Windows, Mac and Linux systems should all work

The best python version is 3.9, other versions should be fine

1. Fill in your openai key in apikey.ini. Note that this code is purely a local project, and your key is safe! If you are not blocked by OpenAI~ There are many novice users, so it might be better if I just give you a screenshot:

2. Ensure global proxy during use! If the client is crashing, you can refer to this for configuration:
3. Installation dependencies: It is best to circumvent the firewall or use domestic sources.

```
pip install -r requirements.txt
```

4.1. Arxiv online batch search + download + summary: Run chat_paper.py, for example:

```
python chat_paper.py -- query "chatgpt robot" -- filter_keys "chatgpt robot" -- max_results 3
```

A more accurate script is chat_arxiv.py, using the scheme, the command line is more concise:

```
python chat_arxiv.py -- query "chatgpt robot" -- page_num 2 -- max_results 3 -- days 10
```

Among them, query is still a keyword, page_num is the search page, each page is the same as the official website, the maximum is 50 articles, max_results is the final summary of the first N articles, and days is the selection of papers in the last few days, strict screening!

**Note: The search term cannot recognize `<code>`- `</code>`, only spaces! So it’s best not to use the hyphen in the original title! `</strong>`Thanks for the information provided by netizens

4.2. Arxiv online batch search + download + summary + advanced search: Run chat_paper.py, for example:

```
python chat_paper.py -- query "all: reinforcement learning robot 2023" -- filter_keys "reinforcement robot" -- max_results 3
```

💥💥💥 7K stars, released a cat girl version of the prompt word, I hope everyone can make cat girls come alive~: [Script: chat_arxiv_maomao.py ](https://github.com/kaixindelele/ChatPaper/blob/main/chat_arxiv_maomao.py), [summary picture](https://github.com/kaixindelele/ChatPaper/blob/main/images/maomao.png)

4.3. Arxiv online batch search + download + summary + advanced search + designated author: Run chat_paper.py, for example:

```
python chat_paper.py -- query "au: Sergey Levine" -- filter_keys "reinforcement robot" -- max_results 3
```

4.4. Local pdf summary: Run chat_paper.py, for example:

```
python chat_paper.py -- pdf_path "demo.pdf"
```

4.5. Batch summary of local folders: Run chat_paper.py, for example:

```
python chat_paper.py -- pdf_path "your_absolute_path"
```

4.6. Google Scholar paper sorting: Run google_scholar_spider.py, for example:

```
python google_scholar_spider.py --kw "deep learning" --nresults 30 --csvpath "./data" --sortby "cit/year" --plotresults 1
```

This command searches Google Scholar for articles related to "deep learning", retrieves 30 results, saves the results to a CSV file in the "./data" folder, sorts the data by the number of citations per year, and plots the results.

For specific usage and parameters, please refer to https://github.com/JessyTsu1/google_scholar_spider

4.7. Gitee image bed configuration tutorial (optional, more troublesome)

Effect and configuration video: https://www.bilibili.com/video/BV1Rh4y1173t/ Tutorial article: https://zhuanlan.zhihu.com/p/644326031

---

Also note that this currently does not support **review **articles.

Station B explanation video: [I open sourced ChatPaper! AI speed reading PDF papers and speed reading Arxiv papers](https://www.bilibili.com/video/BV1EM411x7Tr/)

**Note: key_word is not important, but filter_keys is very important! **Be sure to change it to your keywords.

In addition, you can refer to the following figure for search keywords on arxiv:

1. Parameter introduction:

```
[--pdf_path Whether to read local pdf documents directly? If not set, search and download directly from arxiv] 
[--query Search keywords from the arxiv website, there are some abbreviation examples: all, ti(title), au(author), a query example: all: ChatGPT robot ] 
[--key_word Keywords in your field of interest, not very important] 
[--filter_keys Keywords you need to search in the abstract text, each word must appear to be considered your target paper] 
[- -max_results The maximum number of articles for each search. After the above filtering, it is your target number of papers. Chat only summarizes the filtered papers] [ 
--sort arxiv's sorting method, the default is relevance, it can also be time, arxiv.SortCriterion.LastUpdatedDate or arxiv.SortCriterion.Relevance, do not add quotation marks] 
[--save_image Whether to save images, if you have not registered gitee's image bed, the default is false] 
[--file_format File saving format, the default is markdown md format, or txt] 

parser.add_argument("--pdf_path", type=str, default='', help="if none, the bot will download from arxiv with query") 
parser.add_argument("-- query", type=str, default='all: ChatGPT robot', help="the query string, ti: xx, au: xx, all: xx,") parser.add_argument("--key_word", type= 
str , default='reinforcement learning', help="the key word of user research fields") 
parser.add_argument("--filter_keys", type=str, default='ChatGPT robot', help="the filter key words, summary Every word in must exist before it will be filtered as a target paper") 
parser.add_argument("--max_results", type=int, default=1, help="the maximum number of results") 
parser.add_argument(" --sort", default=arxiv.SortCriterion.Relevance, help="another is arxiv.SortCriterion.LastUpdatedDate") 
parser.add_argument("--save_image", default=False, help="save image? It takes a minute or two to save a picture! But pretty") 
parser.add_argument("--file_format", type=str, default='md', help="The exported file format, if you save a picture, it is best to be md, if not If so, the txt will not be messy")
```

### 2. Run as Flask service

Flask configuration tutorial

Note: After updating the version, there may be path errors.

1. Download the project and enter the project directory

```
git clone https://github.com/kaixindelele/ChatPaper.git 
cd ChatPaper

```

1. Fill in your OpenAI key in the `apikey.ini file `in the project root directory .
2. Configure the virtual environment and download dependencies

```
pip install virtualenv 
Install virtual environment tool 
virtualenv venv 
Create a new virtual environment named venv 
Under Linux/Mac: 
source venv/bin/activate 

Under Windows: 
.\venv\Scripts\activate.bat 

pip install -r requirements.txt

```

1. Start service

```
python3 app.py 
# Start the Flask service. After running this command, the Flask service will start on the local port 5000 and wait for user requests. Access the homepage of the Flask service by visiting one of the following addresses in your browser: 
# http://127.0.0.1:5000/ 
# or 
# http://127.0.0.1:5000/index
```

After visiting http://127.0.0.1:5000/, you will see the homepage. On the homepage, you can click on different links to invoke various services. You can achieve different effects by modifying the parameter values in the link. For parameter details, see the detailed introduction in the previous step

flask main interface

* In particular, these four interfaces are actually web interfaces that encapsulate four scripts in the root directory. Parameters can be modified via links. For example, if you want to run "arxiv?query=GPT-4&key_word=GPT+robot&page_num=1&max_results=1&days=1&sort=web&save_image=False&file_format=md&language=zh", it is equivalent to calling chat_arxiv.py in the root directory and returning the result. The result displayed is the same as the result called from the command line (i.e.: python chat_arxiv.py –query “GPT-4” –key_word “GPT robot” –page_num 1 –max_results 1 –days 1 –sort “web” – save_image False –file_format “md” –language “zh”). You can obtain other search results by modifying the parameters.

If deployed in this way, the results will be saved in the newly generated export, pdf_files and response_file folders in the same directory.

### 3. Run as docker

Docker configuration tutorial details

Note: The path of Docker was also disrupted by me. There may be problems and it is not recommended to try.

1. To install docker and docker-compose, you can refer to the following link

* https://yeasy.gitbook.io/docker_practice/install
* https://yeasy.gitbook.io/docker_practice/compose/install

2. Find a place to put the "docker-compose.yaml" file in the project root directory, and replace `YOUR_KEY_HERE on line 21 `with your own openai_key
3. Run from the command line in the same directory

* `docker-compose up -d`

4. Such an interface represents something normal. Then visit https://127.0.0.1:28460/ and you can open it from the web page! docker-compose

* In particular, if you have ideas for improving the project, you can check the three scripts build.sh, dev.sh, tagpush.sh and the functions of the files in the root docker directory. I believe they will have some influence on your ideas for containerized packaging projects. further improve
* All running results are saved in Docker volumes. If you want to deploy them as a service for a long time, you can map these directories. By default, they are located under /var/lib/docker/volumes/. You can go into this directory and view the results in four related folders: chatpaper_log, chatpaper_export, chatpaper_pdf_files, and chatpaper_response_file. For a detailed explanation of Docker volumes, please refer to this link: http://docker.baoshu.red/data_management/volume.html.

## HuggingFace online deployment

HuggingFace online deployment details

Note: The same is true for this part. The function has been temporarily disabled. It is recommended that you use the web version of chatwithpaper.org directly.

1. Create your own personal account on [Hugging Face and log in;](https://huggingface.co/)
2. Enter the main ChatPaper repository: [https://huggingface.co/spaces/wangrongsheng/ChatPaper ](https://huggingface.co/spaces/wangrongsheng/ChatPaper). You can see all the latest deployment codes in [Files and Version ;](https://huggingface.co/spaces/wangrongsheng/ChatPaper/tree/main)
3. [Optional] Use private deployment: Click [Duplicate this space , ](https://huggingface.co/spaces/wangrongsheng/ChatPaper?duplicate=true)select `Visibility as Private `in the pop-up page , and finally click `Duplicate Space `. The Space code will be deployed to your own Space. For your convenience, you can call it every time There is no need to fill in the API-key. You can modify [app.py#L845 ](https://huggingface.co/spaces/wangrongsheng/ChatPaper/blob/5335124d25b1bc4017a2f5c48b0038dfa545bf63/app.py#L845)to your key: `default="sk-abcdxxxxxxxx" `. Click to save the file and it will be redeployed immediately;
4. [Optional] Use of public deployment: Click [Duplicate this space , ](https://huggingface.co/spaces/wangrongsheng/ChatPaper?duplicate=true)select `Visibility as Public `in the pop-up page , and finally click `Duplicate Space `. The code of the Space will be deployed to your own Space, so that a publicization can be completed deployment.

   Note: You can choose between public deployment and privatized deployment according to your needs!

## Any PDF full text translation configuration tutorial

1. Must be used under Ubuntu or MacOS! The following tutorial defaults to Ubuntu18.04/20.04. It is recommended to use vultr cloud server, which is very worry-free.
2. After installing ChatPaper's default dependencies, activate its virtual environment and enter the scipdf_parser-master folder. After entering this path, continue to install the dependencies here.
3. After installing the following, you still need to install the Java environment. We recommend installing java11.0.19
4. Update the system package first: sudo apt-get update
5. Then command to install OpenJDK 11: sudo apt-get install openjdk-11-jdk
6. After completing the above steps, you can use the following command to confirm the installed Java version: java -version
7. This will return your current Java version information. At this point, Java is basically installed successfully.
8. Then, start the scipdf service in the background. You need to download a lot of dependencies here: bash serve_grobid.sh
9. After the service is started, you can leave it alone, open a new terminal, and start the python program: python chat_summary.py
10. You can also start serve_grobid.sh by default in the background: nohup bash serve_grobid.sh

Finally, I wish you a happy use!

## Local PDF full text translation example

View local PDF full text translation example

# Reinforcement learning, robotics, and simulation-to-real-world transfer

## Reinforcement Learning, Robotics, Sim-to-Real Transfer

## Summary

Current reinforcement learning (RL) algorithms struggle with long-term tasks, where time can be wasted exploring dead ends and task progress can be easily reversed. We developed the SPOT framework, which explores within safe areas of action, learns information about unsafe areas without exploring them, and prioritizes experiences that reverse previous progress to achieve superior learning effects. The SPOT framework successfully simulated a variety of tasks, increasing the benchmark success rate from 13% to 100% when stacking 4 blocks, and from 13% to 100% when creating rows of 4 blocks. Increased to 99%, improves baseline success rate from 84% to 95% when clearing lined toys in hostile mode. Efficiency is typically improved by 30% or more relative to the number of actions per trial, while training requires only 1-20k actions, depending on the task. Furthermore, we demonstrate straightforward simulation-to-real transfer. By loading the simulation-trained model directly on the real robot, without additional real-world fine-tuning, we were able to create realistic stacks with 61% efficiency in 100% of the trials, and realistic rows in 100% of the trials , the efficiency is 59%. To our knowledge, this is the first time that successful simulation-to-real transfer has been applied to long-term multi-step tasks such as stacking blocks and creating rows and taking into account reversal of progress. The code is available at https://github.com/jhulcsr/good_robot. Index terms - computer vision for other robotic applications, deep learning in grasping and manipulation, reinforcement learning. Multi-step robotic tasks are very challenging in real-world environments. They combine the immediate physical consequences of actions with the need to understand how those consequences affect progress toward an overall goal. Furthermore, in contrast to traditional action planning, which assumes perfect information and a known action model, learning can only obtain limited spatial and temporal information from the perceptual environment.

## "Good Robot!": Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer

Figure 1. Stacks and rows of blocks created by the robot, transferred from simulation to reality. Our Forward Planning of Tasks (SPOT) framework can help us efficiently find strategies that can complete multi-step tasks. Video overview: https://youtu.be/MbCuEZadkIw

Our key observation is that reinforcement learning wastes a lot of time exploring behaviors that are, at best, ineffective. For example, in the task of stacking blocks (Figure 1), humans know that grabbing the air will never grab the object. This is "common knowledge", but for ordinary algorithms, it may take some time to discover. To address this problem, we propose the forward planning of tasks (SPOT) framework to incorporate common sense constraints into deep reinforcement learning (DRL) in a way that can significantly accelerate learning and final task efficiency [1], [2].

While these constraints are intuitive, incorporating them into deep reinforcement learning in a way that enables reliable and efficient learning is very difficult. Our approach (Section 3) is inspired by a humane and effective pet training method sometimes called “positive conditioning”. Consider the goal of training a dog named "Spot" to ignore an object or event that is of particular interest to her. When Spot exhibits a partially desired final behavior, she is rewarded, and in the retrograde case, she is immediately removed from the non-rewardable situation. One way to achieve this is to start with multiple rewards in hand, place one reward in Spot's field of vision, and if she eagerly jumps to the reward (a negative behavior), the human will immediately snatch and hide the reward to This behavior will not be rewarded. Through repeated training, Spot will eventually hesitate, at which time she will immediately be praised "Good Spot!" and receive a reward. At the same time, the object she should ignore will also be removed. This approach can be extended to new situations and behaviors, and encourages exploration and rapid improvement once initial partial success is achieved. As we describe in Section 3, our reward function and SPOT-Q learning are also designed to neither reward nor punish retrograde behavior.

Retrograde situations have varying complexities. On the one hand, not being able to stack the first block on top of another puts the robot in a similar situation, so recovery requires Ω(1) actions. However, once a stack of n blocks exists, even a successful grab may push the entire stack over, reversing the entire action history for a given trial (Figure 3), so recovery requires Ω(n) actions. The latter, more dramatic retrograde situation is a challenging problem for reinforcement learning in robots learning multi-step tasks; our work provides an efficient way to address this situation.

In summary, the contributions of this paper include: 1) A SPOT framework for multi-step tasks, which improves existing techniques in simulated environments and enables efficient training in real situations. 2) SPOT-Q learning, a safe and efficient training method in which exploration behavior is focused through masks at runtime and additional on-the-fly training samples are generated from past experience. 3) Zero-shot domain transfer from simulated stacking and row building tasks to real-world environments, and robustness to changes in hardware and scene location. Ideally, the algorithm should be able to efficiently learn to avoid this situation and achieve success via the success metric shown by the green arrow. Therefore, time and workspace dependencies need to be considered. Events at current time ti ∈ T, i ∈ [1…n] can affect the likelihood of successful outcomes for past actions th |h &lt; i and future actions tj |j > i. In our experiments, part of the stack or row itself is a scene obstacle. The gray walls here are for illustrative purposes only. 4) An ablation study shows that context removal significantly reduces retrograde cases; progress metrics improve efficiency; trial rewards improve in discounting, but there is a trade-off between efficiency and support for sparse rewards.

## II. Related work

Deep neural networks (DNNs) enable the use of raw sensor data in robotic operations [1]-[5]. In some methods, the output of DNN directly corresponds to motion instructions, such as [3], [4]. Advanced methods, on the other hand, assume a simple model of robot control and focus on bounding box or pose detection for downstream grasp planning [1], [6]-[11]. RGB-D sensors can bring benefits [1], [11], [12] because they can capture physical information about the workspace. Object-centric skill learning can be effective and have good generalization capabilities, for example [13]–[16] focus on stacking by classifying simulated stacks as stable or likely to collapse. Similarly, [17], [18] develop physical intuition by predicting the consequences of pushing actions. Our work differs in that visual understanding and physical intuition are developed simultaneously during the progression of a multi-step task.

Scraping is a particularly active area of research. DexNet [19], [20] learns from a large number of top-down grasped depth images and shows excellent performance in grasping new objects, but does not consider long-term tasks. 6-DOF Grasp-Net [21] uses simulated grasping data to generalize to new objects, and has been extended to handle reliable grasping of new objects in cluttered environments [12].

Deep reinforcement learning (DRL) has proven its effectiveness in increasingly complex tasks in robot operation [1], [5], [22], [23]. QT-Opt [5] mastered operational skills by learning from hundreds of thousands of grasping attempts on real robots. Domain adaptation, such as applying random textures in simulations, can also enhance transfer from simulation to the real world [24], [25]. Other methods focus on transferring visuomotor skills from simulated robots to real robots [22], [26]. Our work learns pixel-level success probability maps, instead of directly regressing the torque vector, but following previous work [1], [23] to guide the low-level controller to perform actions.

Multi-step tasks with sparse rewards are a special challenge for reinforcement learning because the solution is unlikely to be discovered through random exploration. When available, demonstrations can be an effective way to guide exploration [27]–[29]. Multi-step tasks can be divided into modular subtasks containing sketches [30], while [31] has robot-specific and task-specific learning modules.

Security is crucial for reinforcement learning in many real-world settings [32]–[34]. Preliminary experiments in Section IV-D show that SPOT-Q provides a way to incorporate security into general algorithms based on Q-Learning [35].

We compare the SPOT framework in Sections IV and V with VPG [1], a reinforcement learning-based approach to desktop cleaning tasks that can be trained from images in a few hours on a single robot. VPG is often able to complete adversarial scenarios, such as first pushing a set of closely stacked blocks apart and then grabbing a now separated object. Some recent related work involves tasks with multiple actions: [36] placing a block on top of another block, [37] placing a towel on a rod, [38] emptying a trash can, but not before Both tasks were not long-term tasks, and reversal of progress was never considered (Figure 3).

## III. Methods

We study multi-step tasks with sparse and approximate notions of task progression. Learning can be made more efficient by taking four steps: structuring these problems to capture the invariant properties of the data, applying traditional algorithms to areas where they work best, ensuring rewards are not propagated through failed actions, and introducing a Algorithms that eliminate unnecessary exploration. We will demonstrate our approach in the context of an assembly problem for vision-based robot manipulation.

We construct the problem as a Markov decision process (S, A, P, R), where the state space is S, the action space is A, the transition probability function is P: S×S×A→R, and the reward function is R :S×A→R. This includes a simplifying assumption that equates sensor observations and states. At time step t, the agent observes state s_t and chooses an action a_t according to its policy π: S→A. This action leads to the new state s_t+1 with probability P(s_t+1 | s_t, a_t). Like VPG [1], we use Q-learning to generate deterministic policies for selecting actions. The function Q: S×A→R estimates the expected reward R of an action in a given state, that is, the “quality” of the action. Our policy π selects action a_t as follows: π(s_t) = arg max a∈AQ(s_t, a) (1)

Therefore, the training goal is to learn a Q that maximizes the reward R over time. This is achieved by iteratively minimizing |Q(s_t, a_t) - y_t|, where the target value y_t is: y_t = R(s_t+1, a_t) + γQ(s_t+1, π(s_t+1)) ( 2)

Q-learning is a fundamental algorithm in reinforcement learning, but there are key limitations in its most general form in applications such as robotics, where the space and cost of actions and new trials are very large and efficient exploration can be crucial or even is safety critical. It is also highly dependent on the reward function R, whose definition can cause learning efficiency to differ by orders of magnitude, as we show in Section IV-C, so we start with reward shaping methods.

## A. Reward Shaping

Reward shaping is an effective technique for optimizing rewards R for efficient training of policies [39] and their neural networks. Here, we propose several reward functions for subsequent comparison (Section IV-C), which construct a general reward shaping formula that facilitates efficient learning on a wide range of new tasks, thereby reducing the success reward schedule. of temporary nature.

Assume that each action a is associated with a subtask φ ∈ φ, and we have an indicator function 1 a [s t+1 , at ] that is equal to 1 if the action at succeeds on the subtask φ and 0 otherwise. Similar to VPG [1], our baseline rewards follow this principle and include a subtask weighting function W: Φ → R, weighted according to their subjective difficulty and importance:

R base (s t+1 , at ) = W (φ t )1 a [s t+1 , at ] (3)

Next, we define a sparse and approximate task progress function P: S → R ∈ [0, 1], representing the proportional progress toward the overall goal, where P(st) = 1 indicates task completion. As we told in the story of Spot the dog (Section I), a reversal of progress leads us to situational removal (SR) of the agent, and there is an indicator function 1 SR [st , s t+1 ] if P (s t+1 ) ≥ P(st ), then it is equal to 1, otherwise it is equal to 0. These lead to the new reward function:

R SR (s t+1 , at ) = 1 SR [st , s t+1 ]R base (s t+1 , at ) (4)

RP (s t+1 , at ) = P(s t+1 )R SR (s t+1 , at ) (5)

One advantage of R base, R SR and RP is that each reward function is available "on the fly" after the transition between two states in a trial. However, they do not take into account the possibility that early errors may lead to failure after many steps (Figures 3, 4), so we will develop a reward that can be spread throughout the experiment.

W φ t ∈ {W push =0.1, W grasp =1, W place =1}.

Actions 11-14: The grab and place actions result in a full stack of height 4, completing the experiment. The final R trial at action 14 is 2 × RP. Here for the visibility of the chart, W φ t ∈ {W push = .5, W grasp = 1, W place =1.25}.

During training, we physically reset the environment (Figure 3). We define a related indicator function 1 SR [st , s t+1 ], which is equal to 1 if P(s t+1 ) ≥ P(st ) and equal to 0 otherwise. These lead to the new reward function:

R SR (s t+1 , at ) = 1 SR [st , s t+1 ]R base (s t+1 , at ) (4)

RP (s t+1 , at ) = P(s t+1 )R SR (s t+1 , at ) (5)

One advantage of R base, R SR and RP is that each reward function is available "on the fly" after the transition between two states in a trial. However, they do not take into account the possibility that early errors may lead to failure after many steps (Figures 3, 4), so we will develop a reward that can be spread throughout the experiment.

## B. Situation removal: SPOT trial rewards

Can we pass a reward function that takes into account actions that cause failure at subsequent time steps while training more efficiently than the standard discounted reward function RD, where RD (s t+1 , at ) = γ RD (s t+2 , a t+1 )? Our approach is to prevent reward propagation across failed actions through the concept of context removal: where R* can be any immediate reward function, such as RSR or RP in Section III-A, with N marking the end of the trial , γ is the usual discount factor, set to γ = 0.65.

R trial (s t+1 , at ) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 0, if R * (s t+1 , at ) = 0 2R * (s t+1 , at ), if t = NR * ( s t+1 , at ) + γR trial (s t+2 , a t+1 ), otherwise

The effect of using R trials is that future rewards are only propagated during the time steps of successful completion of the subtask. As shown in Figure 4, and described in the figure caption, the zero reward of context removal cuts off the propagation of future rewards in the time step containing the failed action. This focuses learning on short and successful sequences that complete the task.

## C. SPOT-Q Learning and Dynamic Action Space

In this section, we further leverage prior knowledge about the environment to make simple but powerful assumptions that both reduce invalid attempts and speed up training. Specifically, there are many cases where the failure of certain actions can be easily predicted from the same sensor signals used for Q-learning. To do this, we assume that there exists a predictor M (st , a) → {0, 1} that takes the current state st and an action a and returns 0 if the action definitely fails and 1 otherwise. This is slightly different from the success indicator 1 a [s t+1 , at ], which requires the result s t+1 of the action at to determine success or failure. 4 Using M, we define the dynamic action space M t (A): M t (A) = {a ∈ A|M (st , a) = 1}. (7) In short, M t (A) does not tell us whether a ∈ A is worth executing, but rather whether it is worth exploring. Given a state st, the question becomes how to best utilize M t in training. If π(st ) ∈ M t (A), then π(st ) can be considered a failure for training purposes and we can explore the next best action that is not guaranteed to fail. To formalize this, we introduce SPOT-Q learning, which is a new objective value function that replaces (2): where π M (st ) = arg max a∈M t (A) Q(st , a ). The key is that we backpropagate both the masked action with 0 reward and the unmasked action π M (st ) that the robot actually performed. Algorithm 1 describes how we continuously train from past examples via SPOT-Q and Prioritized Experience Replay (PER) [40] while executing the current policy. In Section IV, we discuss how SPOT-Q allows us to go beyond previous work, where similar heuristics [1], [41] can neither match SPOT-Q nor consider security, which we discuss later sexual considerations.

y M,t = ⎧ ⎨ ⎩ yt , if π(s t+1 ) ∈ M t (A) yt + γQ(s t+1 , π M (s t+1 )) otherwise + R(s t+1 , at ). (8)

## IV. Simulation experiment

Our method improves performance and action efficiency on the desktop cleaning task of VPG [1] as well as two challenging multi-step tasks designed by us. Our best results achieve 100% experimental success in simulated stacking and permutation tasks, and we show that these models successfully transfer to the real world (see Section V for details).

To understand the contribution of each element of our approach to the overall performance, we detail a series of simulation experiments. To this end, we evaluate each reward function, the impact of SPOT-Q on heuristic exploration, other possible SPOT-Q implementations, the reward weighting term W, and we describe the use of SPOT-Q + RP and SPOT-Q + R Best results from trial. Briefly, we found that contextual removal of R SR improved our performance the most, RP improved accuracy and efficiency, and R trial was more efficient than discounting rewards during training, taking into account the time delay between actions and outcomes. . SPOT-Q improves results both with no masking and with only basic masking. Finally, we tested a grid world navigation task [42] to show how the SPOT framework can be applied to safety reinforcement learning. Tables I and III summarize these results.

## A. Robot implementation details

We consider a robot that can be assigned to specific arm poses and gripper states in the workspace. Our action space consists of three components: action type Φ, position X × Y, and angle Θ. The agent views the environment through a fixed RGB-D camera, which we project so that the z-axis is aligned with the direction of gravity, as shown in Figure 2. We discretize the spatial action space into a square height map with side length 0.448m, with 224×224 coordinates (x, y), so each pixel represents approximately 4mm², similar to VPG [1]. The angular space Θ = {2πik | i ∈ [0, k − 1]} is also discretized into k = 16 boxes.

The set of action types includes three high-level motion primitives Φ = {grab, push, place}. In our experiments, the success of the movement is related to the sensors of our gripper, for grasping, to pushing, to the perturbation of the object, and to placing, to an increase in stack height or row length.

A traditional trajectory planner performs each action a = (φ, x, y, θ) ∈ A on the robot. For grab and place, each action moves the robot to (x, y) with a gripper angle θ ∈ Θ and closes or opens the gripper respectively. The pushing action starts from (x, y), the gripper is closed, and moves horizontally a fixed distance along the angle θ. Figure 2 visualizes our overall algorithm, including the action space and corresponding Q-values.

## B. Evaluation indicators

We evaluate our algorithm on random test cases according to the metrics in VPG [1]. Ideal action efficiency is 100%, calculated as the ideal number of actions divided by the actual number of actions; for grasping tasks, it is defined as 1 action per object; for tasks involving placement, it is defined as 2 actions per object. This means that for the stack task with a height of 4, a total of 6 actions are required since only 3 objects need to be moved; for the row task of placing two blocks between two endpoints, a total of 4 actions are required. We verify the simulation results twice through 100 trials with new random object locations.

## C. Algorithm Analysis

We compare the contribution of each component of the underlying algorithm against baseline methods in Table I, except for the cleaning tasks presented in the text. Unless stated otherwise, we summarize rows and stacks into a combined average.

Cleaning 20 toys: We established a benchmark through a major simulation experiment in VPG [1], where 20 toys of various shapes had to be grasped to clean the robot workspace. The SPOT framework matches VPG [1], achieves 100% task completion rate, and increases the crawling success rate from 68% to 84% and the action efficiency from 64% to 74%.

Cleaning Challenging Toys: The second benchmark scenario is an 11 challenging toy layout from VPG [1], where toys are placed in a tightly packed configuration. Running each case 10 times, the SPOT framework cleared completely in 7/11 cases, while only 5/11 cases were cleared in VPG [1]; the clearance rate improved from 84% to 95% across all 110 runs. In this case, the efficiency drops, from 60% to 38%, due to an increase in the number of difficult cases to solve, since separating blocks may require multiple attempts.

Reward functions: R base , R SR , RP and R trial gradually extend each other (Section III-A, Section III-B). Unless otherwise stated, all masking operations were disabled in this study.

RD st RD (s t+1 , at ) = γ RD (s t+2 , a t+1 ) is the most common method of discounting rewards. When evaluated using RP at the last time step and γ = 0.9, the success rates of grab and place actions are 5% and 45% respectively. Create 2-3 layer stacks and improve performance through masking operations (32%, 48%). However, this approach is very inefficient, with no 4-level stack in 20,000 actions. That is, if orders of magnitude more training could be performed, we would expect convergence [43].

R base is efficient for pushing and grabbing [1], but is insufficient for multi-step tasks, only 13% of rows and stacks are completed in the best case, requiring approximately 200+ actions per trial . In another case, it often repeatedly looped grabbing and then placing the same object in the same location, resulting in a 99% grab success rate but no successful trials overall, even after manual scene resets. We don't expect R base to converge on these tasks because there is no progress signal to indicate that, for example, grabbing from the top of the existing stack is a bad choice.

R SR immediately solves the problem of progress reversal since such actions are not rewarded; as a result, we see an increase in trial success from 13% to 94% and an order of magnitude increase in efficiency to 23%, for both tasks , that is, each trial requires approximately 22 actions.

RP resulted in an increase in overall trial success rate to 97% and efficiency to 45%, i.e. approximately 20 actions per trial. This improves upon pure situational elimination by incorporating a quantitative amount of progress into it.

R trial Using RP as the immediate reward function in this test, the stack had an average trial success rate of 96% and an efficiency of 31%, which is approximately 19 actions per trial. However, for rows, performance drops significantly, with trial success rate dropping to 80% and action efficiency only 16%, i.e. approximately 25 actions are required per trial. These values indicate that R trial trades off the inefficiency of RD and the more immediate progress metrics in RP, since the most recent values can be used to fill in actions without progress feedback. We also note that once SPOT-Q is added, this reward is the best reward in the stack, and the second-best reward overall, as shown below.

SPOT-Q: VPG [1] evaluated a heuristic that specifies the exact locations to explore and found that it resulted in performance degradation. Similar methods in QT-Opt [41] are phased out during training, indicating that they do not contribute in improving training results. In contrast, SPOT-Q is always enabled and excludes regions with zero reward possibility while leaving other regions of interest open. So, does this difference in heuristic design matter?

The “Shielded without SPOT-Q” test disables the if statement in Algorithm 1 to simulate a typical heuristic in which exploration is directed to a specific area without zero reward guidance. Compared with the case without shielding, "shielded but without SPOT-Q" completed 95% of the test, compared with 88% without shielding and 99% with SPOT-Q; the results of action efficiency are more obvious , respectively 37%, 23% and 50%. These results and Section IV-D show that SPOT-Q works throughout training and testing with little adjustment, so we conclude that SPOT-Q improves the efficiency of learning from heuristic data.

Alternatives to SPOT-Q: We evaluate two alternatives to SPOT-Q (eq. 8, Algorithm 1), in which all masked pixels undergo 0-reward backpropagation and the masking is applied on the actually performed actions Loss of (1) sum and (2) mean of scores. In both cases, the gradient explodes and the algorithm fails to converge. Only SPOT-Q can effectively improve convergence.

Reward weighting: SPOT-Q + RP, where W push = 0.1, succeeds in 99% of trials, but only has a 27% success rate when W push = 1.0. The impact of weighting on R trials in Figure 4 without masking or SPOT-Q achieved 97% stack success and 38% action efficiency, but for consistency we kept all weighting values constant. This shows that W (3) is important for efficient training.

SPOT-Q + RP: This configuration has the best overall simulation performance, with a trial success rate of 99% and an efficiency of 50%, i.e. approximately 10 actions per trial. It is also the best simulated row model, with a 98% trial success rate in one test and a 100% success rate in the second test, with an action efficiency of 62-68%.

SPOT-Q + R trial: This was the best stack model, achieving 100% in both test cases with an efficiency of 45-51%. The overall performance was the second best, with a trial success rate of 97% and an efficiency of 37%, i.e. approximately 14 actions per trial.

## D. Security and Domain Generalization

To demonstrate the broad applicability of the SPOT framework, we performed evaluations on the simple but challenging Safety Grid World [42] (Figure 5) environment, a type of environment widely used to evaluate reinforcement learning algorithms [32] ,[39]. In this environment, the red robot must move forward or turn to navigate toward the green square without entering the lava. If we only had a real robot learning in the world, standard deep reinforcement learning (DRL) would be very unsafe, but the SPOT framework allows the robot to safely explore the space.

As shown in Table III, all improvements are consistent with our more realistic tasks. We first use Rainbow [35], a Q-learning based DRL method, which can only complete at most 12% of trials out of 500 k actions, with an efficiency of 12%. We then conducted a small ablation study, gradually adding Masking, SPOT-Q, and RP to Rainbow; achieving 96.9%, 95.5%, and 99.9% of 1000 test trials respectively; the average efficiencies were 75%, 73%, and 99.9%, respectively. 62%; the average number of actions to complete 30 verification tests are 123 k, 113 k and 70 k respectively. All failures using the mask did not enter the lava, and they hit the 100-action limit.

These results are consistent with our more realistic experiments, show how the SPOT framework generalizes in completely different scenarios, and illustrate the application of the SPOT framework in security exploration. Next, we show how the SPOT framework can directly apply knowledge gained in simulations to real robotic tasks.

## V. REAL WORLD EXPERIMENTS

Finally, we study the performance of SPOT-Q on real robotic tasks, including training from scratch and simulation-to-real transfer. In both cases, the performance is roughly comparable to that achieved in simulations, demonstrating the advantages of our approach for efficient and effective reinforcement learning. We used the setup described in [29] and [44], including a universal robot UR5, a Robotiq 2-finger gripper, and a Primesense Carmine RGB-D camera; except for the robotic arm, other parts differ from our simulations. Other implementation details are described in Section IV-A, and the results are shown in Table II.

Realistic Pushing and Grasping: We trained benchmark pushing and grasping tasks from scratch in the real world, tested on 20 objects, and showed 100% test pass rate, 75% grasp success rate and 1k times 75% efficiency in actions; these results are comparable to the performance of VPG [1] in 2.5k actions. Simulation-to-real migration was not successful in this task.

## Simulation to real environment and real environment stack comparison

After training in the simulation environment, we directly load the model onto the real robot for execution. Surprisingly, all tested simulation-to-real environment stacked models completed 100% of trials, outperforming models trained on real robots, which succeeded in 82% of trials (Figure 6, Table II ). Action efficiency for the RP and R trials was equal at 61%, while the RP version without SPOT-Q or mask was slightly less efficient at 51%. This is especially impressive, considering our scenes are exposed to changing sunlight. Intuitively, these results are partly due to the use of depth-heightmaps as input in stacking and row production.

Simulated to real environment row production: Our RP + SPOT-Q simulated to real environment row production model can successfully create rows in 100% of attempts, with an efficiency of 59%. R trial + SPOT-Q and RP without mask performed slightly worse, with 90% of trials completed, and efficiencies of 83% and 58% respectively. The high efficiency of RP without masking is due to the fact that we end the real experiment immediately when the task becomes irrecoverable, such as when a block falls from the work area. In this case, we only evaluate simulated-to-real transfer, since training progresses much slower than on stacking tasks.

We expect block-based tasks to be transferable because the network mainly relies on depth images, which are more consistent between simulations and real data. This may reasonably explain why pushing and grabbing cannot be transferred, and this problem can be alleviated by using methods such as domain adaptation in future work [24], [25].

## VI. Conclusion

We have demonstrated that the SPOT framework is effective for training long-term tasks. To our knowledge, this is the first time that reinforcement learning has been successfully applied to long-term multi-step tasks such as stacking blocks and creating rows with progress reversal considerations. The SPOT framework can quantify an agent's progress in multi-step tasks while providing zero-reward guidance, masked action space, and context removal. It can quickly learn strategies from simulation to real-world. We found these methods to be necessary to achieve 100% completion rates for realistic stacking tasks and row crafting tasks.

The main limitation of SPOT is that while intermediate rewards may be sparse, they are still necessary. Future research should explore methods for learning task structure from data, including context removal. Furthermore, the action space mask M is currently designed manually; this mask and lower-level open-loop actions can also be learned. Another topic for study is the difference in successful simulation-to-real transfer between push and grab tasks versus stacking and rowing tasks. Finally, we hope to apply our method to more challenging tasks.

## Acknowledgments

We would like to give special thanks to Adit Murali for integrating Safe Grid World; to Molly O'Brien for valuable discussions, feedback, and editorial comments; to Corinne Hundt for writing the "Good Robot!" title; to Michelle Hundt, Thomas Hundt, and Ian Harkins for editing; thanks to everyone who read, reviewed, and provided feedback; thanks to the authors of VPG[1] for posting their code.

## Local PDF full text summary example

View local PDF full text summary example example

# Reinforcement learning for learning on long-term tasks

## Reinforcement learning for long-horizon tasks

## Abstract

This paper introduces a reinforcement learning algorithm for long-term tasks that avoids wasting time on invalid paths during exploration and can effectively learn to reverse previous progress. We develop the SPOT framework, which explores within action-safe regions, learns information about unsafe regions without actually exploring these regions, and prioritizes reversing previous experience for efficient learning. In simulation experiments, the SPOT framework successfully completed various tasks, increasing the success rate of the benchmark test from 13% to 100% (when stacking 4 blocks), and from 13% to 99% (when creating 4 blocks). row), and from 84% to 95% (when clearing toys arranged in a versus mode). Efficiency is often improved by 30% or more compared to the number of actions per trial, while training time requires only 1-20k actions, depending on the complexity of the task. Furthermore, we demonstrate the ability to transfer directly from simulation to real environments. By loading the simulation-trained model directly on the real robot, without additional real-world fine-tuning, we were able to successfully stack real blocks in 100% of the trials, with an efficiency of 61%, and successfully create real rows in 100% of the trials. , the efficiency is 59%. To our knowledge, this is the first reinforcement learning instance to apply successful simulation-to-real transfer to long-term multi-step tasks (such as stacking blocks and creating rows) and taking into account progress reversal. Code is available at https://github.com/jhulcsr/good_robot. Index terms - Application of computer vision in other robotic applications, Application of deep learning in grasping and manipulation, Reinforcement learning.

## “Good Robot!”: Efficient Reinforcement Learning for Multi-Step Visual Tasks with Sim to Real Transfer

This section introduces a framework called "Schedule for Positive Task (SPOT)" for efficient reinforcement learning in multi-step vision tasks. The authors observed that reinforcement learning often wastes a lot of time exploring behaviors that are ineffective at best. To solve this problem, the authors propose the SPOT framework, which incorporates common sense constraints into deep reinforcement learning, thereby significantly accelerating the learning process and improving task efficiency.

The SPOT framework is inspired by an effective method of training pets, namely "positive conditioning". The authors apply this approach to reinforcement learning to encourage exploration and rapid improvement by rewarding behavior that partially matches the desired behavior and immediately stopping the reward when counter-behavior occurs. The author's reward function and SPOT-Q learning method are also designed not to reward or punish adverse behavior.

In multi-step tasks, the complexity of the inverse behavior varies. For simple tasks like stacking the first block on top of another, returning to the original state only takes a few moves. However, once there is a stack consisting of n blocks, even if one block is successfully grabbed, the entire stack may be knocked over, causing all previous actions to be reversed, and the recovery complexity will be Ω(n). This more complex inverse behavior is a challenging problem for reinforcement learning of multi-step tasks in robots, and our work provides an efficient way to solve this situation.

The contributions of this article include: 1) The SPOT framework is used for reinforcement learning multi-step tasks and can be effectively trained in both simulated and real environments. 2) SPOT-Q learning method, a safe and efficient training method that improves efficiency through runtime exploration and generation of additional training samples from past experience. 3) Zero-sample domain transfer is achieved in simulated and real environments, as well as robustness to hardware and scene location changes. 4) The efficiency of progress is improved by eliminating inverse behavior; the efficiency is improved by introducing progress measures; the discount method is improved by experimental rewards, but there is a trade-off between efficiency and sparse reward support.

In summary, the SPOT framework proposed in this article has made significant progress in reinforcement learning of multi-step visual tasks, improving learning efficiency and task effectiveness.

## II. RELATED WORK (related work)

This section presents the work related to this study. First, the application of deep neural networks (DNNs) enables the utilization of raw sensor data in robot operations. In some methods, the output of DNN directly corresponds to the movement instructions. While other advanced methods assume simple models of robot control and focus on bounding box or pose detection for downstream grasp planning. RGB-D sensors can provide physical information about the workspace. Goal-centered skill learning can be effectively and broadly generalized, for example by classifying simulated stacks as stable or likely to collapse. Similarly, there are ways to develop physical intuition by predicting the consequences of pushing actions. The present study differs from these approaches in that visual understanding and physical intuition are developed simultaneously during the progression of a multi-step task.

Scraping is a particularly active area of research. DexNet learns a large number of top-down grasping depth images and shows excellent performance in grasping new objects, but does not consider long-term tasks. 6-DOF Grasp-Net uses simulated crawl data to generalize to new objects and has been extended to handle reliable crawling of new objects in cluttered environments.

Reinforcement learning (DRL) has proven its effectiveness in increasingly complex tasks in robot operation. QT-Opt learned its operating skills from hundreds of thousands of actual grasping attempts on real robots. Domain adaptation, such as applying random textures in simulations, can also enhance transfer from simulation to the real world. Other approaches focus on the transfer of visuomotor skills from simulated robots to real robots. This study follows previous work by learning pixel-level success probability maps by guiding low-level controllers to perform actions rather than directly regressing torque vectors.

In reinforcement learning, sparse rewards for multi-step tasks present a special challenge because the solution is unlikely to be discovered through random exploration. If a demo is available, it can be an effective way to guide exploration. Multi-step tasks can be divided into modular subtasks containing sketches, while [31] has robot-specific and task-specific learning modules.

Safety is critical to reinforcement learning in many real-world settings. Preliminary experiments in Section 4 show that SPOT-Q provides a way to incorporate security into general algorithms based on Q-Learning.

We compare the SPOT framework with VPG in Sections IV and V. VPG is a reinforcement learning-based approach to desktop cleaning tasks that can be trained on images in a few hours on a single robot. VPG is often able to complete adversarial scenarios, such as first pushing a set of closely stacked blocks apart and then grabbing the now separated object. Some recent related work involves tasks with multiple actions, among which [36] placing a block on top of another, [37] placing a towel on a pole, [38] cleaning a trash can, but the first two None were long-term missions, and reversal of progress was never considered (Figure 3).

## III. APPROACH (method)

We study the notion of sparse and approximate task progression in long-term tasks. The efficiency of learning can be improved by following four measures: structuring these problems to capture the invariant properties of the data, using traditional algorithms to deploy them where they work best, ensuring that rewards are not propagated through failed actions, and introducing a method that can eliminate Algorithms that require no exploration. We will demonstrate our approach in the context of assembly problems in vision-based robotic manipulation.

We construct the problem as a Markov decision process (S, A, P, R), where the state space is S, the action space is A, the transition probability function is P: S × S × A → R, and the reward function is R : S × A → R. This involves a simplifying assumption that equates sensor observations and states. At time step t, the agent observes state s_t and chooses action a_t according to its policy π: S → A. The probability that this action leads to the new state s_t+1 is P(s_t+1 | s_t, a_t). Similar to VPG [1], we use Q-learning to generate deterministic policies for selecting actions. The function Q: S × A → R estimates the expected reward R of choosing an action from a given state, i.e. the "quality" of the action. Our policy π selects action a_t as follows: π(s_t) = arg max a∈AQ(s_t, a) (1)

Therefore, the training goal is to learn a Q that maximizes the reward R. This is achieved by iteratively minimizing |Q(s_t, a_t) - y_t|, where the target value y_t is: y_t = R(s_t+1, a_t) + γQ(s_t+1, π(s_t+1)) ( 2)

Q-learning is a fundamental algorithm in reinforcement learning, but has some key limitations when applied to areas such as robotics where the action space and experimentation costs are extremely high, and efficient exploration may even be safety-critical. It is also highly dependent on the reward function R, whose definition may cause the learning efficiency to differ by several orders of magnitude, which we show in Section IV-C, so we first introduce the reward shaping method.

## A. Reward Shaping

Reward shaping is an effective technique for optimizing rewards R for efficient training of policies [39] and their neural networks. Here, we propose several reward functions for subsequent comparison (Section IV-C), which establish a general form of reward shaping that facilitates efficient learning on a wide range of new tasks, thereby reducing success rewards. Temporary nature of the plan.

Suppose each action a is associated with a subtask φ ∈ Φ, and we have an indicator function 1 a [s t+1 , at ] that is equal to 1 if the action at succeeds on φ and 0 otherwise. Similar to VPG [1], our baseline rewards follow this principle and include a subtask weighting function W : Φ → R, weighted according to their subjective difficulty and importance:

R base (s t+1 , at ) = W (φ t )1 a [s t+1 , at ].

Next, we define a sparse and approximate task progress function P : S → R ∈ [0, 1], representing the proportional progress toward the overall goal, where P(st) = 1 indicates task completion. As in our Spot the dog story (Section I), the reversal of progress leads us to situational removal (SR) of the agent and a physical reset of the environment during training (Figure 3). We define a related indicator function 1 SR [st , s t+1 ], which is equal to 1 if P(s t+1 ) ≥ P(st ) and equal to 0 otherwise. These lead to the new reward function:

R SR (s t+1 , at ) = 1 SR [st , s t+1 ]R base (s t+1 , at ).

RP (s t+1 , at ) = P(s t+1 )R SR (s t+1 , at ).

An advantage of R base, R SR and RP is that they are all available "on the fly" after two state transitions in a trial. However, they do not take into account the possibility that early errors may lead to failure many steps later (Figures 3, 4), so we will develop a reward that can be spread throughout the experiment.

W φ t ∈ {W push =0.1, W grasp =1, W place =1}.

W φ t ∈ {W push = .5, W grasp = 1, W place =1.25} for chart visibility.

R trial at a 14 is 2 × R P.

R SR (s t+1 , at ) = 1 SR [st , s t+1 ]R base (s t+1 , at ).

RP (s t+1 , at ) = P(s t+1 )R SR (s t+1 , at ).

## B. Situation Removal: SPOT Trial Reward (Situation Removal: SPOT Trial Reward)

This section discusses whether the reward function can take into account actions that lead to failure at subsequent time steps when the training efficiency is higher than the standard discounted reward R_D(s_t+1, a_t) = γ R_D(s_t+2, a_t+1). Our approach is to prevent reward propagation across failed actions through the concept of context removal, where R* can be an arbitrary immediate reward function, such as R_SR or R_P from Section III-A, and N marks the end of the trial, γ is the usual discount factor, set to γ = 0.65.

The effect of using R_trial is that future rewards are only propagated in time steps where subtasks are successfully completed. As shown in Figure 4 and described in the caption, the zero reward for context removal cuts off the propagation of future rewards in the time step containing the failed action. This focuses learning on short and successful sequences to complete the task.

## C. SPOT-Q Learning and Dynamic Action Space

In this section, we further leverage prior knowledge about the environment to make simple but powerful assumptions that both reduce invalid attempts and speed up training. Specifically, there are many cases where certain action failures can be easily predicted from the same sensor signals used for Q-learning. To do this, we assume that there exists an oracle M(s_t, a) → {0, 1}, which takes the current state s_t and an action a, and returns 0 if the action will definitely fail, and 1 otherwise. This is slightly different from the success indicator 1_a[s_t+1, a_t], which requires the result s_t+1 of action a_t to determine success or failure. Using M, we define the dynamic action space M_t(A): M_t(A) = {a ∈ A|M(s_t, a) = 1}. (7)

In short, M_t(A) does not tell us whether a ∈ A is worth executing, but rather whether it is worth exploring. Given a state s_t, the question becomes how to best utilize M_t in training. If π(s_t) ∈ M_t(A), then π(s_t) can be considered a failure for learning purposes and we can explore the next action that is most likely not to fail. To formalize this, we introduce SPOT-Q learning, which is a new objective value function that replaces (2): where π_M(s_t) = arg max a∈M_t(A) Q(s_t, a). The key is that we backpropagate both the masked actions with 0 rewards and the unmasked actions π_M(s_t) actually performed by the robot. Algorithm 1 describes how we continuously train from past examples via SPOT-Q and Prioritized Experience Replay (PER) [40], while also showing the execution of the current policy. In Section 4, we discuss how SPOT-Q surpasses previous work, where similar heuristics [1], [41] neither match SPOT-Q nor take into account the security considerations we discuss later .

y_M,t = ⎧ ⎨ ⎩ y_t, if π(s_t+1) ∈ M_t(A) y_t + γQ(s_t+1, π_M(s_t+1)), otherwise + R(s_t+1, a_t). (8)

## IV. SIMULATION EXPERIMENTS

This section presents a series of simulation experiments to understand the contribution of each element of our approach to the overall performance. We evaluate each reward function, the impact of SPOT-Q on heuristic exploration, other possible SPOT-Q implementations, the reward weighting term W, and describe our results on SPOT-Q + R_P and SPOT-Q + R_trial Best results. In short, we found that context removal R_SR improved our performance the most, R_P improved accuracy and efficiency, and R_trial trained better while accounting for the time delay between actions and consequences. SPOT-Q improves results relative to both unmasked and basic masking. Finally, we tested a grid world navigation task [42] to show how the SPOT framework can be applied to safety reinforcement learning. Tables I and III summarize these results.

(Our method improves performance and action efficiency over the state of the art on the table clearing task from VPG [1], as well as on two challenging multi-step tasks of our design: creating a stack of four blocks and creating a horizontal row of four blocks. Our best results can achieve 100% trial success on the simulated stacking and row tasks, models which successfully transfer to the real world as we show in Section V.)

## A. Robot Implementation Details (机器人实施细节)

We consider a robot that can be assigned to specific arm poses and gripper states within its workspace. Our action space consists of three components: action type Φ, position X × Y, and angle Θ. The agent views the environment through a fixed RGB-D camera, which we project so that the z-axis is aligned with the direction of gravity, as shown in Figure 2. We discretize the spatial action space into a square height map with side length 0.448m, with 224×224 coordinates (x, y), so each pixel represents approximately 4mm², according to VPG [1]. The angle space Θ = {2πik | i ∈ [0, k-1]} is also discretized into k = 16 bins.

The set of action types includes three high-level motion primitives Φ = {grab, push, place}. In our experiments, the success of actions was related to the sensors of our gripper for grasping, the perturbation of the object for pushing, and the increase in stack height or row length for placement.

A traditional trajectory planner performs each action a = (φ, x, y, θ) ∈ A on the robot. For grab and place, each action will move to (x, y) with a gripper angle θ ∈ Θ and close or open the gripper respectively. The pushing action starts with the closed gripper at (x, y) and moves a fixed distance horizontally along the angle θ. Figure 2 visualizes our overall algorithm, including the action space and corresponding Q-values.

## B. Evaluation Metrics

We evaluate our algorithm on random test cases following the metrics in VPG [1]. The ideal action efficiency is 100%, calculated by dividing the ideal number of actions by the actual number of actions. For grasping tasks, only 1 action is required per object; for tasks involving placement, 2 actions per object are required. For example, for the stacking task with a height of 4, only 3 objects need to be moved, so a total of 6 actions are required; for the row task of placing two blocks between two endpoints, a total of 4 actions are required. We verify the simulation results twice by running 100 random trials of new object locations.

[1] VPG: Virtual-to-Physical Robot Grasping.

## C. Algorithm Analysis

This section compares the contribution of each component of the underlying algorithm with baseline methods through the comparison in Table I. In addition to the cleaning tasks provided in the text, we summarize the lines and stacks into an average.

* Cleaning 20 toys: We established a benchmark through a major simulation experiment found in VPG [1], where 20 toys of various shapes had to be grasped to clean the robot workspace. The SPOT framework fully matches VPG [1], and the task completion rate increases from 68% to 84%, and the crawling success rate increases from 64% to 74%.
* Adversarial Cleaning Toys: The second benchmark scenario is 11 challenging adversarial layouts from VPG [1] in which toys are placed in a closely packed configuration. Running each case 10 times, the SPOT framework completely cleared 7/11 cases, compared with only 5/11 cases in VPG [1]; the clearance rate improved from 84% to 95% for all 110 runs. In this case, the efficiency drops from 60% to 38%, which is due to the increased number of difficult cases to solve, since separating blocks may require multiple attempts.
* Reward functions: R base, R SR, RP and R trial gradually extend each other (Section III-A, Section III-B). All masks were disabled in this study unless otherwise stated.
* RD, that is, RD (s t+1 , at ) = γ RD (s t+2 , a t+1 ), is a conventional experimental reward method. When evaluated using RP at the last time step and γ = 0.9, the success rates of grab and place actions are 5% and 45% respectively. Creating a 2-3 layer stack and using masks improves performance (32%, 48%). However, this approach is very inefficient, with 20,000 actions without a 4-layer stack. Nonetheless, if orders of magnitude more training could be performed, we would expect convergence [43].
* R base is efficient for pushing and grabbing [1], but not sufficient for multi-step tasks. In the best case, only 13% of the rows and stacks were completed, requiring about 200 moves per trial. In another case, it frequently pushed and placed the same object repeatedly, resulting in a 99% grasp success rate but no successful trials overall, even after manually resetting the scene. We don't expect R base to converge on these tasks because there is no progress signal that grabbing from the top of the existing stack is a bad choice.
* R SR immediately solves the problem of progress reversal, since such an action gets a reward of 0; thus, we see that the trial success rate increases from 13% to 94%, and the efficiency increases by an order of magnitude, to 23%, for both tasks, That is, each trial requires approximately 22 actions.
* RP resulted in an increase in trial success rate to 97% and efficiency to 45%, i.e. approximately 20 actions per trial. This approach improves upon pure situational elimination by incorporating a quantitative amount of progress.
* R trial used RP as the immediate reward function in this test, and the average trial success rate of the stack was 96% and the efficiency was 31%, which is about 19 actions per trial. However, for rows, performance drops significantly, with trial success rate dropping to 80% and action efficiency only 16%, i.e. approximately 25 actions are required per trial. These values indicate that R trial trades off the inefficiency of RD and the more immediate progression in RP, since the most recent values can be used to fill in actions without progress feedback. We also noticed that once SPOT-Q is added, this bonus is the best in the stack and the second best overall, as shown below.
* SPOT-Q: VPG [1] evaluated a heuristic that specifies the exact locations to explore and found that it resulted in worse performance. In QT-Opt [41], similar methods are eliminated during training, indicating that they do not contribute to improving training results. In contrast, SPOT-Q is always enabled and eliminates areas with no likelihood of success, while other areas of interest can still be explored. So, does this difference in heuristic design matter?
* The “Mask without SPOT-Q” test disables the if statement in Algorithm 1 to simulate a typical heuristic where exploration is directed to a specific area without the guidance of zero reward. Compared with no mask and no SPOT-Q, "mask but no SPOT-Q" completed 95% of the trials, with action efficiencies of 37%, 23% and 50% respectively. These results and Section IV-D show that SPOT-Q works effectively throughout training and testing with little adjustment, so we conclude that SPOT-Q improves the efficiency of learning from heuristic data .
* Alternatives to SPOT-Q: We evaluate two alternatives to SPOT-Q (eq. 8, Algorithm 1), in which 0-reward backpropagation is performed on all masked pixels and on the masked The (1) sum and (2) average of the scores apply the loss. In both cases, the gradient explodes and the algorithm fails to converge. Only SPOT-Q can effectively improve the convergence speed.
* Reward weighting: SPOT-Q + RP, where W push = 0.1, succeeds in 99% of trials, but when W push = 1.0, the success rate is only 27%. Without masking or SPOT-Q, the impact of weighting on R trials in Figure 4 resulted in a stack success rate of 97% and an action efficiency of 38%, but for consistency we left all weightings unchanged . This shows that W (3) is important for efficient training.
* SPOT-Q + RP: This configuration has the best overall simulation performance, with a test success rate of 99% and an efficiency of 50%, which means each test requires approximately 10 actions. It was also the best simulated row model, with a trial success rate of 98% in one test and 100% in the second, and an action efficiency of 62-68%.
* SPOT-Q + R trial: This was the best stack model, achieving 100% in both test cases with an efficiency of 45-51%. Overall performance was the second best, with a trial success rate of 97% and an efficiency of 37%, i.e. approximately 14 actions per trial.

## Full text summary example

View full text summary results

Paper:1

Title: Diffusion Policy: Visuomotor Policy Learning via Action Diffusion Chinese title: Visuomotor Policy Learning via Action Diffusion

Authors: Haonan Lu, Yufeng Yuan, Daohua Xie, Kai Wang, Baoxiong Jia, Shuaijun Chen

Affiliation: Central South University

Keywords: Diffusion Policy, Visuomotor Policy, robot learning, denoising diffusion process

Urls: http://arxiv.org/abs/2303.04137v1, Github: None

Summary:

(1): This article studies the learning of robot visual action strategies. The learning of robot visual action strategy refers to outputting corresponding robot motion actions based on the observed information. This task is relatively complex and challenging.

(2): Past methods include using different action representation methods such as Gaussian mixture models, classification representation, or switching strategy representation, but there are still challenging problems such as multimodal distribution and high-dimensional output space. This paper proposes a new robot visual motion strategy model - Diffusion Policy, which combines the expression ability of the diffusion model, overcomes the limitations of traditional methods, can express arbitrary distributions and supports high-dimensional space. This model learns the gradient of the cost function, uses the stochastic Langevin dynamics algorithm for iterative optimization, and finally outputs the robot action.

(3): The robot visual action strategy proposed in this article - Diffusion Policy, represents the robot action as a conditional denoising diffusion process. This model can overcome problems such as multimodal distribution and high-dimensional output space, and improves the expressive ability of policy learning. At the same time, this paper continues to enhance the performance of the diffusion strategy by introducing technologies such as outlook control, visual induction, and time series diffusion transformation.

(4): Our method was tested on 11 tasks, including 4 robot manipulation benchmarks. Experimental results show that Diffusion Policy shows obvious superiority and stability compared to existing robot learning methods, with an average performance improvement of 46.9%.

7.Methods: The visual action policy learning method proposed in this article, namely Diffusion Policy, includes the following steps:

1. Establish a conditional denoising diffusion process: a conditional random diffusion process that represents the robot action as a source containing Gaussian noise. In this process, the robot state serves as the source, that is, the input, and the robot's motion actions are output through the diffusion process. To turn this into a conditional stochastic diffusion model, we include a cost function, which acts as a condition in the path integral.
   1. Introducing stochastic Langevin dynamics: converting the gradient of the learning cost function into an iterative optimization problem based on stochastic Langevin dynamics. This method avoids explicit computational diffusion processes and satisfies the requirements of a derivative-free optimizer, benefiting from asymptotic Gaussian properties as well as global convergence properties.
      1. Introducing diffusion strategy enhancement technology: using outlook control technology, combined with the decision-making network, to adjust the actions generated by diffusion to enhance the performance of the strategy. At the same time, visual induction and time series diffusion transformation are introduced to further improve the expression ability of the diffusion strategy.

         1. Tested on 11 tasks: The test results show that compared with existing robot learning methods, this method shows obvious superiority and stability in robot manipulation benchmark tests, with an average performance improvement of 46.9%.

         * 7.Conclusion:
         * (1): This paper studies the learning method of robot visual movement strategy, and proposes a new robot visual movement strategy model - Diffusion Policy. By introducing the expression ability of the diffusion model, it overcomes the limitations of traditional methods and can express arbitrary distributions. And supports high-dimensional space. Experimental results show that this method shows obvious superiority and stability on 11 tasks. Compared with existing robot learning methods, the average performance is improved by 46.9%. This research is of great significance.
         * (2): Although this paper proposes a new robot visual action strategy learning method and achieved good performance in experiments, the optimization process of this method may be time-consuming. In addition, the performance of this method is affected by many factors, including the quality and quantity of demonstrations, the physical capabilities of the robot, and the strategy architecture, etc. These factors need to be considered in actual application scenarios.
         * (3): If I were to recommend it, I would give this article 9 points. The Diffusion Policy method proposed in this article has the advantages of high interpretability, good performance, and stable experimental results, and can bring great inspiration and reference to fields such as robot visual action strategy learning. The only drawback may be that the optimization process of the method requires more time and effort.

## skills

View tips

Quickly review papers with specific keywords. If there are no illustrations, each article will take one minute, and the reading time will be almost one minute.

This project can be used to track the latest papers in the field, or pay attention to papers in other fields. It can generate summaries in batches, up to 1000 (if you can wait). Although Chat may be made up, within the framework of my standardized questions, its main information is well-established.

The digital part requires everyone to re-check the original text!

After you find a good article, you can read it carefully.

We recommend two other AI-assisted websites for intensive reading of papers: https://typeset.io/ and chatpdf. My tutorial: [Enhanced Apprenticeship: Evaluation of SciSpace (Typeset.io), the paper reading tool - Evolving with AI](https://zhuanlan.zhihu.com/p/611874187)

The main advantage of the above two tools is that ChatPaper can automatically summarize the latest papers in batches, which can greatly lower the reading threshold, especially for us Chinese. The shortcomings are also obvious. ChatPaper does not have interactive functions and cannot ask questions continuously, but I think this is not important~

## Common errors

View common errors

1. pip installation error:

<p id="gdcalert3" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image3.jpg). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert4">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

![alt_text](images/image3.jpg "image_tooltip")

It is recommended to turn off the ladder and use domestic sources to download:

```
pip install -r requirements.txt -i http://pypi.douban.com/simple --trusted-host pypi.douban.com

```

1. APIConnectionError occurs when calling openai's chatgpt api. How to solve it? Refer to Zhihu answer: https://www.zhihu.com/question/587322263/answer/2919916984

Add it directly to chat_paper.py

os.environ[“http_proxy”] = “http://:” os.environ[“https_proxy”] = “http://:”

You need to find the proxy IP and port in the Windows system.

1. Error report when the API is banned by OpenAI:

<p id="gdcalert4" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image4.jpg). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert5">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

![alt_text](images/image4.jpg "image_tooltip")

_3222_

In this case, you can only use a new account. In addition, you must pay attention to not using the same account too many times, and the node must be reliable. You must not use nodes in mainland China and Hong Kong. If you use it, send it.

1. HTTPS communication error:

<p id="gdcalert5" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image5.jpg). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert6">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

![alt_text](images/image5.jpg "image_tooltip")

This error is most likely due to the node not being clean enough. If anyone knows the specific reason, please open an issue.

[issue174 ](https://github.com/kaixindelele/ChatPaper/issues/174)is:

```
pip install urllib3 == 1.25.11
```

## Project Acknowledgments

1. Thanks to the lab for its support and guidance, the group friends and lab classmates for their technical support and a lot of forwarding! There are also bosses Zhang and Hua’s advice.
2. [Siyuan ](https://github.com/HouSiyuan2001)shared two core functions with me when I started the project, which saved a lot of time.
3. [Rongsheng ](https://github.com/WangRongsheng)’s online website allows more technical novices to try this project.
4. A useful arxiv paper download package provided by the author of [Arxiv .](https://github.com/lukasschwab/arxiv.py)
5. [PyMuPDF ](https://github.com/pymupdf/PyMuPDF)provides good PDF parsing tools. Let the entire information flow be opened up.
6. OpenAI provides such a powerful AI model, which makes the entire AI industry come alive and provides a foundation for the construction of the academic "Tower of Babel".
7. Thanks to the author of Ex-ChatGPT for sharing various ChatGPT development details. I learned a lot during the development process and have been developing our web version content now. In addition, for computer professionals, I recommend this very powerful open source tool:
8. Thanks to the author of ChatReviewer for merging his project into our ChatPaper, making ChatPaper more complete. [Ex-ChatGPT ](https://github.com/circlestarzero/EX-chatGPT)is a powerful tool platform that enables ChatGPT to call external APIs such as WolframAlpha, Google and WikiMedia to provide more accurate and timely answers. People in the world call it GoogleChat.
9. We also have to thank GitHub officials for helping our project be listed [at fifth on the hot list ](https://github.com/trending)and gaining a lot of attention!
10. Later, we will open up the entire project process. We need to thank the students who are also from the Chinese Academy of Sciences for developing [gpt_academic ](https://github.com/binary-husky/gpt_academic). We made polishing parts based on their work. and [nishiwen1214 ](https://github.com/nishiwen1214)’s [ChatReviewer ](https://github.com/nishiwen1214/ChatReviewer), which completed our review and review responses.
11. Thanks to the CCF-A paper database provided by [SilenceEagle ](https://github.com/SilenceEagle/paper_downloader), we have summarized 30,000+ papers offline.
12. Thanks to the back room community for open source and organizing the Chinese data set [MNBVC ](https://github.com/esbatmop/MNBVC), and hope that the domestic Chinese large model will take off soon!
13. Thanks to all the friends and teachers who have supported the project and helped me along the way!

## Starchart

## Contributors

## 项目引用：

Please cite the repo if you use the data or code in this repo.

```
@misc{ChatPaper,
  author={Yongle Luo, Rongsheng Wang, Peter Gam, Jiaxi Cui, circlestarzero, Shiwen Ni, Jaseon Quanta, Qingxu Fu, Siyuan Hou},
  title = {ChatPaper: Use LLM to summarize papers.},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/kaixindelele/ChatPaper}},
}
```
